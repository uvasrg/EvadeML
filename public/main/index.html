<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <title>
    EvadeML: Evading Machine Learning Classifiers // EvadeML
  </title>

  <link href="http://gmpg.org/xfn/11" rel="profile">
<meta http-equiv="content-type" content="text/html; charset=utf-8">


<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="">
<meta name="generator" content="Hugo 0.17" />

  <meta property="og:title" content="EvadeML: Evading Machine Learning Classifiers" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:locale" content="en_US" />
<meta property="og:url" content="//evademl.org/main/" />


  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/base-min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/pure-min.css">
  
  
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.5.0/grids-responsive-min.css">
  
  

  <link rel="stylesheet" href="//evademl.org/css/srg.css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  <link href='//fonts.googleapis.com/css?family=Open+Sans:400,400italic,200,100,700,300,500,600,800' rel='stylesheet' type='text/css'>
  <link href='//fonts.googleapis.com/css?family=Libre+Baskerville:400,700,400italic' rel='stylesheet' type='text/css'>

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/rotunda.png">
  <link rel="shortcut icon" href="/rotunda.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="EvadeML" />

    
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/tomorrow-night-bright.min.css">
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>


  

  

  
</head>

<body>
	

	<div id="layout" class="pure-g">
    <div class="sidebar pure-u-1 pure-u-md-1-4">
  <div class="header">
    <p class="brand-group">

<a href="https://www.cs.virginia.edu/yanjun/gQdata.htm">Maching Learning Group</a><br>
and <a href="http://www.jeffersonswheel.org">Security Research Group</a><br>
<a href="http://www.cs.virginia.edu">University of Virginia</a>
</p>



    <a href="//evademl.org"><h1 class="brand-title">EvadeML</h1></a>
    <p class="brand-tagline">Machine Learning in the Presence of Adversaries</p>



    

  </div>
</div>

	
	

    <div class="content pure-u-1 pure-u-md-3-4">
		<a name="top"></a>
		

		
  		<section class="post">
            <h1 class="post-title">
              <a href="/main/">EvadeML: Evading Machine Learning Classifiers</a>
            </h1>
            <h3 class="post-subtitle">
            	
            </h3>
            
            	
            

			
			

			

			

            

<h1 id="is-robust-machine-learning-possible">Is Robust Machine Learning Possible?</h1>

<p>Machine learning has shown remarkable success in solving complex
classification problems, but current machine learning techniques
produce models that are vulnerable to adversaries who may wish to
confuse them, especially when used for security applications like
malware classification.</p>

<p><img align="right" src="/images/mlassumption.png" width=600></p>

<p>The key assumption of machine learning is that a model that is trained
on training data will perform well in deployment because the training
data is representative of the data that will be seen when the
classifier is deployed.</p>

<p>When machine learning classifiers are used in security applications,
however, adversaries may be able to generate samples that exploit the
invalidity of this assumption.</p>

<p>Our project is focused on understanding, evaluating, and improving the
effectiveness of machine learning methods in the presence of motivated
and sophisticated adversaries.</p>

<h2 id="projects">Projects</h2>

<section style="display: table;width: 100%">
  <header style="display: table-row; padding: 0.5rem">
    <div style="display: table-cell; padding: 0.5rem; color:#FFFFFF;background:#663399;text-align: center;width: 49%">
<a href="/gpevasion" class="hlink">Genetic&nbsp;Programming</a>
    </div>
        <div style="display: table-cell; padding: 0.5rem;color:#000000;background: #FFFFFF;text-align: center; width:2%""></div>
    <div style="display: table-cell; padding: 0.5rem;color:#FFFFFF;background: #2c0f52;text-align: center;">
<a href="/squeezing" class="hlink">Feature Squeezing</a>
    </div>
  </header>
  <div style="display: table-row;">
    <div style="display: table-cell;">
    <a href="/gpevasion"><img src="/images/geneticsearch.png" alt="Genetic Search" width="100%" align="center"></a><br>
Evolutionary framework to automatically find variants that preserve malicious behavior but evade a target classifier.
    </div>
    <div style="display: table-cell;"></div>
    <div style="display: table-cell;text-align:center">
    <a href="/squeezing"><img src="/images/squeezing.png" alt="Feature Squeezing" width="100%" align="center"></a><br>
Reducing the search space for adversaries by coalescing inputs.<br>
<font size="-1" style="color:#666;">(The top row shows L<sub>0</sub> adversarial examples, squeezed by median smoothing.)</font>
</div>
  </div>
</section>

<h2 id="papers">Papers</h2>

<p>Weilin Xu, David Evans, Yanjun Qi. <a href="/docs/featuresqueezing.pdf"><em>Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</em></a>.
<a href="https://www.ndss-symposium.org/ndss2018/"><em>2018 Network and Distributed System Security Symposium</em></a>. 18-21 February, San Diego, California. Full paper (15 pages): [<a href="/docs/featuresqueezing.pdf">PDF</a>]</p>

<p>Weilin Xu, David Evans, Yanjun Qi. <a href="https://arxiv.org/abs/1705.10686"><em>Feature Squeezing Mitigates and Detects
Carlini/Wagner Adversarial Examples</em></a>. arXiv preprint, 30 May 2017. [<a href="https://arxiv.org/pdf/1705.10686.pdf">PDF</a>, 3 pages]</p>

<p>Weilin Xu, Yanjun Qi, and David Evans. <a href="/docs/evademl.pdf"><em>Automatically Evading
Classifiers A Case Study on PDF Malware Classifiers</em></a>.  <a href="https://www.internetsociety.org/events/ndss-symposium-2016"><em>Network and Distributed Systems Symposium 2016</em></a>, 21-24 February 2016, San Diego, California. Full paper (15 pages): [<a href="/docs/evademl.pdf">PDF</a>]</p>

<h2 id="talks">Talks</h2>

<p><center>
<script async class="speakerdeck-embed" data-id="cdfcf454436240e4ab1a6c4d594e5c7a" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script>
</center><br>
Weilin Xu&rsquo;s talk at <a href="http://www.ndss-symposium.org/ndss2018/">Network and Distributed System Security Symposium 2018</a>. San Diego, CA. 21 February 2018.
<center></p>

<p><center>
<script async class="speakerdeck-embed" data-id="450d6c5f23dd452b8504ac4b8c1bbf84" data-ratio="1.77777777777778" src="//speakerdeck.com/assets/embed.js"></script><br>
David Evans&rsquo; Talk at <a href="https://www.icsi.berkeley.edu/icsi/events/2017/06/adversarial-machine-learning">Berkeley ICSI</a>, 8 June 2017.
</center></p>

<p><cener>
<iframe width="640" height="360" src="https://www.youtube.com/embed/XYJamxDROOs" frameborder="0" allowfullscreen></iframe><br>
David Evans&rsquo; Talk at <a href="https://www.usenix.org/conference/enigma2017/conference-program/presentation/evans">USENIX Enigma 2017</a>, Oakland, CA, 1 February 2017. [<A href="https://speakerdeck.com/evansuva/classifiers-under-attack-1">Speaker Deck</a>]</br>
</center></p>

<p><a href="talks/">More Talks&hellip;</a></p>

<h2 id="code">Code</h2>

<p><strong>EvadeML-Zoo:</strong> <a href="https://github.com/mzweilin/EvadeML-Zoo">https://github.com/mzweilin/EvadeML-Zoo</a></p>

<p><strong>Genetic Evasion:</strong> <a href="https://github.com/uvasrg/EvadeML">https://github.com/uvasrg/EvadeML</a> (Weilin Xu)</p>

<p><strong>Feature Squeezing:</strong> <a href="https://github.com/uvasrg/FeatureSqueezing">https://github.com/uvasrg/FeatureSqueezing</a> (Weilin Xu) (supersceded by the EvadeML-Zoo toolkit)</p>

<p><strong>Adversarial Learning Playground</strong>: <a href="https://github.com/QData/AdversarialDNN-Playground">https://github.com/QData/AdversarialDNN-Playground</a> (Andrew Norton) (mostly supersceded by the EvadeML-Zoo toolkit)</p>

<h2 id="team">Team</h2>

<p><a href="http://www.cs.virginia.edu/~wx4ed/">Weilin Xu</a> (Lead PhD Student, leading work on <a href="/squeezing">Feature Squeezing</a> and <a href="/gpevasion">Genetic Evasion</a>)<br />
Anant Kharkar (Undergraduate Researcher working on <a href="/gpevasion">Genetic Evasion</a>)<br />
<a href="http://www.noahdkim.com/">Noah Kim</a> (Undergraduate Researcher working on <a href="/zoo">EvadeML-Zoo</a>)<br />
Helen Simecek (Undergraduate Researcher working on <a href="/gpevasion">Genetic Evasion</a>)</p>

<p><a href="https://www.cs.virginia.edu/evans">David Evans</a> (Faculty Co-Advisor)<br />
<a href="https://www.cs.virginia.edu/yanjun/">Yanjun Qi</a> (Faculty Co-Advisor)</p>

	
			

			

          </section>
          
          	
          
        
      <div class="footer">
	<hr class="thin" />


	<p></p>
</div>

    </div>
  </div>
	

	

  
</body>
</html>
