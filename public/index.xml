<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>EvadeML</title>
    <link>//evademl.org/index.xml</link>
    <description>Recent content on EvadeML</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="//evademl.org/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Automatically Evading Classifiers</title>
      <link>//evademl.org/gpevasion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/gpevasion/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;EvadeML&lt;/strong&gt; is an evolutionary framework based on genetic programming
  for automatically finding variants that evade detection by machine
  learning-based malware classifiers.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//evademl.org/images/method.png&#34;&gt;&lt;img src=&#34;//evademl.org/images/method.png&#34; alt=&#34;Overview&#34; width=&#34;650px&#34; height=&#34;199px&#34;&gt;&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;
Machine learning is widely used to develop classifiers for security
tasks. However, the robustness of these methods against motivated
adversaries is uncertain. In this work, we propose a generic method to
evaluate the robustness of classifiers under attack. The key idea is to
stochastically manipulate a malicious sample to find a variant that
preserves the malicious behavior but is classified as benign by the
classifier. We present a general approach to search for evasive variants
and report on results from experiments using our techniques against two
PDF malware classifiers, PDFrate and Hidost.&lt;/p&gt;

&lt;p&gt;Our method is able to automatically find evasive variants for both
classifiers for all of the 500 malicious seeds in our study. Our results
suggest a general method for evaluating classifiers used in security
applications, and raise serious doubts about the effectiveness of
classifiers based on superficial features in the presence of
adversaries.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;
&lt;a href=&#34;//evademl.org/images/accumulated_evasion_by_trace_length.png&#34;&gt;&lt;img src=&#34;//evademl.org/images/accumulated_evasion_by_trace_length.png&#34; alt=&#34;Overview&#34; width=&#34;531px&#34; height=&#34;369px&#34;&gt;&lt;/a&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;h3 id=&#34;paper&#34;&gt;Paper&lt;/h3&gt;

&lt;p&gt;Weilin Xu, Yanjun Qi, and David Evans. &lt;a href=&#34;//evademl.org/docs/evademl.pdf&#34;&gt;&lt;em&gt;Automatically Evading
Classifiers A Case Study on PDF Malware Classifiers&lt;/em&gt;&lt;/a&gt;.  &lt;a href=&#34;https://www.internetsociety.org/events/ndss-symposium-2016&#34;&gt;&lt;em&gt;Network and
Distributed Systems Symposium
2016&lt;/em&gt;&lt;/a&gt;,
21-24 February 2016, San Diego, California. Full paper (15 pages): [&lt;a href=&#34;//evademl.org/docs/evademl.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;

&lt;h3 id=&#34;talks&#34;&gt;Talks&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://youtu.be/XYJamxDROOs&#34;&gt;&lt;strong&gt;Classifiers Under Attack&lt;/strong&gt;&lt;/a&gt;, David Evans&amp;rsquo; talk at &lt;a href=&#34;https://www.usenix.org/conference/enigma2017/conference-program/presentation/evans&#34;&gt;&lt;em&gt;USENIX Enigma 2017&lt;/em&gt;&lt;/a&gt; (1 February 2017)&lt;br /&gt;
&lt;a href=&#34;a href=&amp;quot;http://www.jeffersonswheel.org/2016/ndss-talk-automatically-evading-classifiers-including-gmails&amp;quot;&#34;&gt;&lt;strong&gt;Automatically Evading Classifiers&lt;/strong&gt;&lt;/a&gt;, Weilin Xu&amp;rsquo;s talk at NDSS 2016 (24 February 2016)&lt;br /&gt;
&lt;a href=&#34;//evademl.org/talks&#34;&gt;More talks&amp;hellip;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;source-code&#34;&gt;Source Code&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/uvasrg/EvadeML&#34;&gt;https://github.com/uvasrg/EvadeML&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;team&#34;&gt;Team&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cs.virginia.edu/~wx4ed/&#34;&gt;Weilin Xu&lt;/a&gt; (Lead PhD Student)&lt;br /&gt;
Anant Kharkar (Undergraduate Researcher)&lt;br /&gt;
Helen Simecek (Undergraduate Researcher)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.virginia.edu/yanjun/&#34;&gt;Yanjun Qi&lt;/a&gt; (Faculty Co-Advisor)&lt;br /&gt;
&lt;a href=&#34;https://www.cs.virginia.edu/evans&#34;&gt;David Evans&lt;/a&gt; (Faculty Co-Advisor)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EvadeML: Evading Machine Learning Classifiers</title>
      <link>//evademl.org/main/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/main/</guid>
      <description>

&lt;h1 id=&#34;is-robust-machine-learning-possible&#34;&gt;Is Robust Machine Learning Possible?&lt;/h1&gt;

&lt;p&gt;Machine learning has shown remarkable success in solving complex
classification problems, but current machine learning techniques
produce models that are vulnerable to adversaries who may wish to
confuse them, especially when used for security applications like
malware classification.&lt;/p&gt;

&lt;p&gt;&lt;img align=&#34;right&#34; src=&#34;//evademl.org/images/mlassumption.png&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;The key assumption of machine learning is that a model that is trained
on training data will perform well in deployment because the training
data is representative of the data that will be seen when the
classifier is deployed.&lt;/p&gt;

&lt;p&gt;When machine learning classifiers are used in security applications,
however, adversaries may be able to generate samples that exploit the
invalidity of this assumption.&lt;/p&gt;

&lt;p&gt;Our project is focused on understanding, evaluating, and improving the
effectiveness of machine learning methods in the presence of motivated
and sophisticated adversaries.&lt;/p&gt;

&lt;h2 id=&#34;projects&#34;&gt;Projects&lt;/h2&gt;

&lt;section style=&#34;display: table;width: 100%&#34;&gt;
  &lt;header style=&#34;display: table-row; padding: 0.5rem&#34;&gt;
    &lt;div style=&#34;display: table-cell; padding: 0.5rem; color:#FFFFFF;background:#663399;text-align: center;width: 49%&#34;&gt;
&lt;a href=&#34;//evademl.org/gpevasion&#34; class=&#34;hlink&#34;&gt;Genetic&amp;nbsp;Programming&lt;/a&gt;
    &lt;/div&gt;
        &lt;div style=&#34;display: table-cell; padding: 0.5rem;color:#000000;background: #FFFFFF;text-align: center; width:2%&#34;&#34;&gt;&lt;/div&gt;
    &lt;div style=&#34;display: table-cell; padding: 0.5rem;color:#FFFFFF;background: #2c0f52;text-align: center;&#34;&gt;
&lt;a href=&#34;//evademl.org/squeezing&#34; class=&#34;hlink&#34;&gt;Feature Squeezing&lt;/a&gt;
    &lt;/div&gt;
  &lt;/header&gt;
  &lt;div style=&#34;display: table-row;&#34;&gt;
    &lt;div style=&#34;display: table-cell;&#34;&gt;
    &lt;a href=&#34;//evademl.org/gpevasion&#34;&gt;&lt;img src=&#34;//evademl.org/images/geneticsearch.png&#34; alt=&#34;Genetic Search&#34; width=&#34;100%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;br&gt;
Evolutionary framework to automatically find variants that preserve malicious behavior but evade a target classifier.
    &lt;/div&gt;
    &lt;div style=&#34;display: table-cell;&#34;&gt;&lt;/div&gt;
    &lt;div style=&#34;display: table-cell;text-align:center&#34;&gt;
    &lt;a href=&#34;//evademl.org/squeezing&#34;&gt;&lt;img src=&#34;//evademl.org/images/squeezing.png&#34; alt=&#34;Feature Squeezing&#34; width=&#34;100%&#34; align=&#34;center&#34;&gt;&lt;/a&gt;&lt;br&gt;
Reducing the search space for adversaries by coalescing inputs.&lt;br&gt;
&lt;font size=&#34;-1&#34; style=&#34;color:#666;&#34;&gt;(The top row shows L&lt;sub&gt;0&lt;/sub&gt; adversarial examples, squeezed by median smoothing.)&lt;/font&gt;
&lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;

&lt;h2 id=&#34;papers&#34;&gt;Papers&lt;/h2&gt;

&lt;p&gt;Weilin Xu, David Evans, Yanjun Qi. &lt;a href=&#34;https://arxiv.org/abs/1704.01155&#34;&gt;&lt;em&gt;Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks&lt;/em&gt;&lt;/a&gt;. arXiv preprint, 4 April 2017. [&lt;a href=&#34;https://arxiv.org/pdf/1704.01155.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;Weilin Xu, Yanjun Qi, and David Evans. &lt;a href=&#34;//evademl.org/docs/evademl.pdf&#34;&gt;&lt;em&gt;Automatically Evading
Classifiers A Case Study on PDF Malware Classifiers&lt;/em&gt;&lt;/a&gt;.  &lt;a href=&#34;https://www.internetsociety.org/events/ndss-symposium-2016&#34;&gt;&lt;em&gt;Network and Distributed Systems Symposium 2016&lt;/em&gt;&lt;/a&gt;, 21-24 February 2016, San Diego, California. Full paper (15 pages): [&lt;a href=&#34;//evademl.org/docs/evademl.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;

&lt;h2 id=&#34;talks&#34;&gt;Talks&lt;/h2&gt;

&lt;p&gt;&lt;iframe width=&#34;640&#34; height=&#34;360&#34; src=&#34;https://www.youtube.com/embed/XYJamxDROOs&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;&lt;br&gt;
David Evans&amp;rsquo; Talk at &lt;a href=&#34;https://www.usenix.org/conference/enigma2017/conference-program/presentation/evans&#34;&gt;USENIX Enigma 2017&lt;/a&gt;, Oakland, CA, 1 February 2017. [&lt;A href=&#34;https://speakerdeck.com/evansuva/classifiers-under-attack-1&#34;&gt;Speaker Deck&lt;/a&gt;]&lt;/br&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;talks/&#34;&gt;More Talks&amp;hellip;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/uvasrg/EvadeML&#34;&gt;https://github.com/uvasrg/EvadeML&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://github.com/uvasrg/FeatureSqueezing&#34;&gt;https://github.com/uvasrg/FeatureSqueezing&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;team&#34;&gt;Team&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cs.virginia.edu/~wx4ed/&#34;&gt;Weilin Xu&lt;/a&gt; (Lead PhD Student, leading work on &lt;a href=&#34;//evademl.org/squeezing&#34;&gt;Feature Squeezing&lt;/a&gt; and &lt;a href=&#34;//evademl.org/gpevasion&#34;&gt;Genetic Evasion&lt;/a&gt;)&lt;br /&gt;
Anant Kharkar (Undergraduate Researcher working on &lt;a href=&#34;//evademl.org/gpevasion&#34;&gt;Genetic Evasion&lt;/a&gt;)&lt;br /&gt;
Helen Simecek (Undergraduate Researcher working on &lt;a href=&#34;//evademl.org/gpevasion&#34;&gt;Genetic Evasion&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.virginia.edu/evans&#34;&gt;David Evans&lt;/a&gt; (Faculty Co-Advisor)&lt;br /&gt;
&lt;a href=&#34;https://www.cs.virginia.edu/yanjun/&#34;&gt;Yanjun Qi&lt;/a&gt; (Faculty Co-Advisor)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Feature Squeezing</title>
      <link>//evademl.org/squeezing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/squeezing/</guid>
      <description>

&lt;h2 id=&#34;detecting-adversarial-examples-in-deep-neural-networks&#34;&gt;Detecting Adversarial Examples in Deep Neural Networks&lt;/h2&gt;

&lt;p&gt;Although deep neural networks (DNNs) have achieved great success in
many computer vision tasks, recent studies have shown they are
vulnerable to adversarial examples.  Such examples, typically
generated by adding small but purposeful distortions, can frequently
fool DNN models. Previous studies to defend against adversarial
examples mostly focused on refining the DNN models. They have either
shown limited success or suffer from expensive computation. We propose
a new strategy, &lt;em&gt;feature squeezing&lt;/em&gt;, that can be used to harden DNN
models by detecting adversarial examples. Feature squeezing reduces
the search space available to an adversary by coalescing samples that
correspond to many different feature vectors in the original space
into a single sample.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;//evademl.org/images/squeezing.png&#34; width=&#34;50%&#34; align=&#34;center&#34;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;By comparing a DNN modelâ€™s prediction on the original input with that
on the squeezed input, feature squeezing detects adversarial examples
with high accuracy and few false positives.  If the original and
squeezed examples produce substantially different outputs from the
model, the input is likely to be adversarial. By measuring the
disagreement among predictions and selecting a threshold value, our
system outputs the correct prediction for legitimate examples and
rejects adversarial inputs.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;//evademl.org/images/squeezingframework.png&#34; width=&#34;75%&#34; align=&#34;center&#34;&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;So far, we have explored two instances of feature squeezing: reducing
the color bit depth of each pixel and smoothing using a spatial
filter. These strategies are straightforward, inexpensive, and
complementary to defensive methods that operate on the underlying
model, such as adversarial training.&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;img src=&#34;//evademl.org/images/jointdetection.png&#34; width=&#34;75%&#34; align=&#34;center&#34;&gt;&lt;br&gt;
&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;The figure shows the histogram of the &lt;em&gt;L&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt; scores on
the MNIST dataset between the original and squeezed sample, for 1000
non-adversarial examples as well as 1000 adversarial examples
generated using both the Fast Gradient Sign Method and the
Jacobian-based Saliency Map Approach. Over the full MNIST testing set,
the detection accuracy is 99.74% (only 22 out of 5000 fast positives).&lt;/p&gt;

&lt;h3 id=&#34;paper&#34;&gt;Paper&lt;/h3&gt;

&lt;p&gt;Weilin Xu, David Evans, Yanjun Qi. &lt;a href=&#34;https://arxiv.org/abs/1704.01155&#34;&gt;&lt;em&gt;Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks&lt;/em&gt;&lt;/a&gt;. arXiv preprint, 4 April 2017. [&lt;a href=&#34;https://arxiv.org/pdf/1704.01155.pdf&#34;&gt;PDF&lt;/a&gt;]&lt;/p&gt;

&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/uvasrg/FeatureSqueezing&#34;&gt;https://github.com/uvasrg/FeatureSqueezing&lt;/a&gt; (includes all the code needed to reproduce the experiments in the paper)&lt;/p&gt;

&lt;h2 id=&#34;team&#34;&gt;Team&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.cs.virginia.edu/~wx4ed/&#34;&gt;Weilin Xu&lt;/a&gt; (Lead PhD Student)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cs.virginia.edu/yanjun/&#34;&gt;Yanjun Qi&lt;/a&gt; (Facutly Co-Advisor)&lt;br /&gt;
&lt;a href=&#34;https://www.cs.virginia.edu/evans&#34;&gt;David Evans&lt;/a&gt; (Faculty Co-Advisor)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Talks</title>
      <link>//evademl.org/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//evademl.org/talks/</guid>
      <description>

&lt;p&gt;Selected talks about our work on adversarial machine learning.&lt;/p&gt;

&lt;h3 id=&#34;classifiers-under-attack&#34;&gt;Classifiers Under Attack&lt;/h3&gt;

&lt;p&gt;&lt;iframe width=&#34;640&#34; height=&#34;360&#34; src=&#34;https://www.youtube.com/embed/XYJamxDROOs&#34; frameborder=&#34;0&#34; allowfullscreen align=&#34;center&#34;&gt;&lt;/iframe&gt;&lt;br&gt;
David Evans&amp;rsquo; Talk at &lt;a href=&#34;https://www.usenix.org/conference/enigma2017/conference-program/presentation/evans&#34;&gt;USENIX Enigma 2017&lt;/a&gt;, Oakland, CA, 1 February 2017. [&lt;A href=&#34;https://speakerdeck.com/evansuva/classifiers-under-attack-1&#34;&gt;Speaker Deck&lt;/a&gt;]&lt;/br&gt;
 &lt;/p&gt;&lt;/p&gt;

&lt;p&gt;
&lt;video width=&#34;640&#34; source src=&#34;https://www.cs.virginia.edu/evans/talks/oreilly.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;br&gt;
David Evans&#39; Talk at &lt;a href=&#34;http://conferences.oreilly.com/security/network-data-security-ny/public/schedule/detail/53176&#34;&gt;O&#39;Reilly Security 2016&lt;/a&gt;, New York City, 2 November 2016. [&lt;a href=&#34;https://speakerdeck.com/evansuva/classifiers-under-attack&#34;&gt;Speaker Deck&lt;/a&gt;]&lt;br&gt;
&lt;/p&gt;

&lt;h3 id=&#34;automatically-evading-classifiers&#34;&gt;Automatically Evading Classifiers&lt;/h3&gt;

&lt;p&gt;&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;0a82f51fd6534cdbb58f3df1bcbc004f&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;&lt;br&gt;
&lt;b&gt;Weilin Xu&amp;rsquo;s Talk at NDSS 2016&lt;/b&gt; (&lt;a href=&#34;http://www.jeffersonswheel.org/2016/ndss-talk-automatically-evading-classifiers-including-gmails&#34;&gt;Blog Post)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>